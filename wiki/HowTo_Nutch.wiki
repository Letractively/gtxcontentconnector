#This page describes how you can use nutch to crawl a website and generate a lucene index that can be used by the gentics content connector.


= Requirements =

  * Index (Crawl) Server - (hint: use linux ;))
  * The latest [http://www.apache.org/dyn/closer.cgi/lucene/nutch/ nutch release].


= Setup =
Connect to your linux index server and navigate to a directory where you want to install nutch. Then download the latest release.

{{{
//Change the url for your needs
wget http://mirror.deri.at/apache/lucene/nutch/nutch-1.0.tar.gz
}}}

Then you have to extract the archive

{{{
tar xzf nutch-1.0.tar.gz
}}}

= Configuration of Nutch =
Editing the nutch configuration is very simple. You only have to follow these small steps:

== Configure the urls to be fetched ==
First you have to create a custom directory that holds the urls you want to index. Navigate to your nutch base directory and do the following:

{{{
mkdir urls
cd urls
vim nutch
}}}

Now you should have opened the file nutch in your new urls directory. There you have to specify the entry page where the crawler should start.

e.g. we want to index http://www.gentics.com => contents of nutch
{{{
http://www.gentics.com
}}}

Open the file /conf/crawl-urlfilter.txt and edit the filter for the urls you want to allow the crawler to follow.

We only change the filter to allow all subdomains of gentics.com and leave everything else as it is.
{{{
# accept hosts in MY.DOMAIN.NAME
+^http://([a-z0-9]*\.)gentics.com/

}}}

== Configure the crawlers identity ==
Now we have to defind the crawlers identity that will be sent to the server as user agent.

Open the file /conf/nutch-site.xml and insert something like the following:
{{{
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
  <name>http.agent.name</name>
  <value>nutch</value>
  <description>HTTP 'User-Agent' request header. MUST NOT be empty -
  please set this to a single word uniquely related to your organization.

  NOTE: You should also check other related properties:

        http.robots.agents
        http.agent.description
        http.agent.url
        http.agent.email
        http.agent.version

  and set their values appropriately.

  </description>
</property>

<property>
  <name>http.agent.description</name>
  <value>nutch index crawler</value>
  <description>Further description of our bot- this text is used in
  the User-Agent header.  It appears in parenthesis after the agent name.
  </description>
</property>

<property>
  <name>http.agent.url</name>
  <value>your.website</value>
  <description>A URL to advertise in the User-Agent header.  This will
   appear in parenthesis after the agent name. Custom dictates that this
   should be a URL of a page explaining the purpose and behavior of this
   crawler.
  </description>
</property>
<property>
  <name>http.agent.email</name>
  <value>your.email.com</value>
  <description>An email address to advertise in the HTTP 'From' request
   header and User-Agent header. A good practice is to mangle this
   address (e.g. 'info at example dot com') to avoid spamming.
  </description>
</property>
</configuration>

}}}

== Highlighted content ==
In order to have a highlighted text snipped available you have to set the content attribute to be stored in the index. Therefore open the file /conf/schema.xml in the Nutch configuration and change the value for the field stored from false to true.
{{{
  <!-- change  -->
        
  <field name="content" type="text" stored="false" indexed="true"/>

  <!-- to -->
  <field name="content" type="text" stored="true" indexed="true"/>

}}}

= Start crawling =
Once you configured everything you can test your configuration with a shallow depth and a low number of links to follow:

execute the command from your nutch base directory
{{{
bin/nutch crawl urls -dir crawl -depth 3 -topN 50
}}}

If you are happy with the results returned, you can increase the values. A common depth value for a full crawl would be around 10 and topN can range from hundreds to hundreds of thousands depending on the sources you want to index/crawl.

= More =
  * You can find more information about nutch on the [http://lucene.apache.org/nutch/ project homepage].